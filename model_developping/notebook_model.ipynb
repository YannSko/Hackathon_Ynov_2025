{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developpement Modèle depuis la bdd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_13764\\4023745716.py:35: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  table_df = pd.read_sql_query(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe shape: (141327, 34)\n",
      "Columns: Index(['roleid', 'nom', 'table_name', 'profilid', 'prenom', 'datedenaissance',\n",
      "       'email', 'entreprise', 'image', 'distancetravailmaison',\n",
      "       'progressionid', 'defiid', 'score', 'description', 'objectif',\n",
      "       'begesid', 'datedebut', 'datefin', 'co2total', 'globalrank',\n",
      "       'transportid', 'co2parkm', 'transportemissionid', 'co2emistransport',\n",
      "       'rank', 'chauffageid', 'co2parm', 'chauffageemissionid',\n",
      "       'co2emischauffage', 'alimentid', 'co2par100g', 'massekg',\n",
      "       'alimentationemissionid', 'co2emisalimentation'],\n",
      "      dtype='object')\n",
      "   roleid      nom table_name  profilid    prenom datedenaissance  \\\n",
      "0     1.0    Admin       role       NaN       NaN             NaN   \n",
      "1     2.0     User       role       NaN       NaN             NaN   \n",
      "2     3.0  Manager       role       NaN       NaN             NaN   \n",
      "3     4.0  Employé       role       NaN       NaN             NaN   \n",
      "4     4.0   User_1     profil       1.0  Prenom_1      1971-03-07   \n",
      "\n",
      "               email    entreprise image  distancetravailmaison  ...  rank  \\\n",
      "0                NaN           NaN   NaN                    NaN  ...   NaN   \n",
      "1                NaN           NaN   NaN                    NaN  ...   NaN   \n",
      "2                NaN           NaN   NaN                    NaN  ...   NaN   \n",
      "3                NaN           NaN   NaN                    NaN  ...   NaN   \n",
      "4  user1@example.com  Entreprise_6  None               4.510607  ...   NaN   \n",
      "\n",
      "   chauffageid  co2parm chauffageemissionid  co2emischauffage  alimentid  \\\n",
      "0          NaN      NaN                 NaN               NaN        NaN   \n",
      "1          NaN      NaN                 NaN               NaN        NaN   \n",
      "2          NaN      NaN                 NaN               NaN        NaN   \n",
      "3          NaN      NaN                 NaN               NaN        NaN   \n",
      "4          NaN      NaN                 NaN               NaN        NaN   \n",
      "\n",
      "  co2par100g massekg  alimentationemissionid  co2emisalimentation  \n",
      "0        NaN     NaN                     NaN                  NaN  \n",
      "1        NaN     NaN                     NaN                  NaN  \n",
      "2        NaN     NaN                     NaN                  NaN  \n",
      "3        NaN     NaN                     NaN                  NaN  \n",
      "4        NaN     NaN                     NaN                  NaN  \n",
      "\n",
      "[5 rows x 34 columns]\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Database configuration\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"ClashOfRse\",\n",
    "    \"user\": \"your_username\",\n",
    "    \"password\": \"your_password\",\n",
    "}\n",
    "\n",
    "def connect_to_db():\n",
    "    \"\"\"Connect to the PostgreSQL database.\"\"\"\n",
    "    return psycopg2.connect(**DB_CONFIG)\n",
    "\n",
    "def fetch_all_tables_data():\n",
    "    \"\"\"Fetch all tables from the database and store them in a single DataFrame.\"\"\"\n",
    "    try:\n",
    "        connection = connect_to_db()\n",
    "        cursor = connection.cursor()\n",
    "\n",
    "        # Fetch all table names\n",
    "        cursor.execute(\"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\")\n",
    "        tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "        if not tables:\n",
    "            raise ValueError(\"No tables found in the database.\")\n",
    "\n",
    "        # Dictionary to store each table's data as a DataFrame\n",
    "        table_dataframes = {}\n",
    "\n",
    "        # Fetch data from each table\n",
    "        for table in tables:\n",
    "            query = f\"SELECT * FROM {table}\"\n",
    "            table_df = pd.read_sql_query(query, connection)\n",
    "            table_dataframes[table] = table_df\n",
    "\n",
    "        # Combine all dataframes into one\n",
    "        combined_df = pd.concat(\n",
    "            [\n",
    "                df.assign(table_name=table)  # Add a column indicating the table name\n",
    "                for table, df in table_dataframes.items()\n",
    "            ],\n",
    "            ignore_index=True\n",
    "        )\n",
    "\n",
    "        print(\"Dataframe shape:\", combined_df.shape)\n",
    "        print(\"Columns:\", combined_df.columns)\n",
    "\n",
    "        return combined_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if connection:\n",
    "            connection.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = fetch_all_tables_data()\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de valeurs manquantes (NaN) par colonne :\n",
      "roleid                    141223\n",
      "nom                       141126\n",
      "table_name                     0\n",
      "profilid                  138849\n",
      "prenom                    141227\n",
      "datedenaissance           141227\n",
      "email                     141227\n",
      "entreprise                141227\n",
      "image                     141327\n",
      "distancetravailmaison     141227\n",
      "progressionid             140349\n",
      "defiid                    140299\n",
      "score                     140349\n",
      "description               141277\n",
      "objectif                  141277\n",
      "begesid                     1179\n",
      "datedebut                   1179\n",
      "datefin                     1179\n",
      "co2total                  139927\n",
      "globalrank                139927\n",
      "transportid                56930\n",
      "co2parkm                  141300\n",
      "transportemissionid        56957\n",
      "co2emistransport           56957\n",
      "rank                        2579\n",
      "chauffageid                99119\n",
      "co2parm                   141319\n",
      "chauffageemissionid        99127\n",
      "co2emischauffage           99127\n",
      "alimentid                 129137\n",
      "co2par100g                141315\n",
      "massekg                   141315\n",
      "alimentationemissionid    129149\n",
      "co2emisalimentation       129149\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Compter les valeurs manquantes (NaN) par colonne\n",
    "nan_counts = df.isna().sum()\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Nombre de valeurs manquantes (NaN) par colonne :\")\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing table: Role\n",
      "Analyzing table: Profil\n",
      "Analyzing table: Transport\n",
      "Analyzing table: Aliment\n",
      "Analyzing table: Chauffage\n",
      "Analyzing table: TransportEmission\n",
      "Analyzing table: AlimentationEmission\n",
      "Analyzing table: ChauffageEmission\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Yann\\AppData\\Local\\Temp\\ipykernel_13764\\853424244.py:19: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  return pd.read_sql_query(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing table: BEGES\n",
      "Analyzing table: Defi\n",
      "Analyzing table: ProgressionDefi\n",
      "\n",
      "Missing Data Summary:\n",
      "\n",
      "Proposed Corrections:\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "\n",
    "# Database configuration\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"ClashOfRse\",\n",
    "    \"user\": \"your_username\",\n",
    "    \"password\": \"your_password\",\n",
    "}\n",
    "\n",
    "def connect_to_db():\n",
    "    \"\"\"Connect to the PostgreSQL database.\"\"\"\n",
    "    return psycopg2.connect(**DB_CONFIG)\n",
    "\n",
    "def fetch_table_data(table_name, connection):\n",
    "    \"\"\"Fetch data from a specific table.\"\"\"\n",
    "    query = f\"SELECT * FROM {table_name}\"\n",
    "    return pd.read_sql_query(query, connection)\n",
    "\n",
    "def identify_missing_data():\n",
    "    \"\"\"Identify missing data in all tables and propose corrections.\"\"\"\n",
    "    try:\n",
    "        connection = connect_to_db()\n",
    "        tables = [\"Role\", \"Profil\", \"Transport\", \"Aliment\", \"Chauffage\", \"TransportEmission\",\n",
    "                  \"AlimentationEmission\", \"ChauffageEmission\", \"BEGES\", \"Defi\", \"ProgressionDefi\"]\n",
    "        missing_data_summary = {}\n",
    "\n",
    "        for table in tables:\n",
    "            print(f\"Analyzing table: {table}\")\n",
    "            df = fetch_table_data(table, connection)\n",
    "            missing_counts = df.isnull().sum()\n",
    "\n",
    "            # Log missing counts\n",
    "            missing_data_summary[table] = missing_counts[missing_counts > 0].to_dict()\n",
    "\n",
    "        # Print summary of missing data\n",
    "        print(\"\\nMissing Data Summary:\")\n",
    "        for table, missing_columns in missing_data_summary.items():\n",
    "            if missing_columns:\n",
    "                print(f\"- {table}:\")\n",
    "                for column, count in missing_columns.items():\n",
    "                    print(f\"    {column}: {count} missing values\")\n",
    "\n",
    "        return missing_data_summary\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        if connection:\n",
    "            connection.close()\n",
    "\n",
    "def propose_corrections(missing_data_summary):\n",
    "    \"\"\"Propose corrections for missing data.\"\"\"\n",
    "    corrections = {}\n",
    "\n",
    "    for table, missing_columns in missing_data_summary.items():\n",
    "        for column in missing_columns:\n",
    "            if \"Id\" in column:\n",
    "                corrections[(table, column)] = \"Ensure referenced table is populated and foreign keys are valid.\"\n",
    "            elif \"Nom\" in column or \"Prenom\" in column:\n",
    "                corrections[(table, column)] = \"Generate random names or use placeholders.\"\n",
    "            elif \"CO2\" in column:\n",
    "                corrections[(table, column)] = \"Impute with average or default CO2 values.\"\n",
    "            elif \"Date\" in column:\n",
    "                corrections[(table, column)] = \"Fill with default dates or approximate ranges.\"\n",
    "            elif \"Score\" in column or \"Rank\" in column:\n",
    "                corrections[(table, column)] = \"Impute with median or random valid scores/ranks.\"\n",
    "            else:\n",
    "                corrections[(table, column)] = \"Analyze and determine logical imputation values.\"\n",
    "\n",
    "    # Print corrections\n",
    "    print(\"\\nProposed Corrections:\")\n",
    "    for (table, column), correction in corrections.items():\n",
    "        print(f\"- {table}.{column}: {correction}\")\n",
    "\n",
    "    return corrections\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    missing_data_summary = identify_missing_data()\n",
    "    corrections = propose_corrections(missing_data_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-21 01:21:16,022 [INFO]: Successfully connected to the database.\n",
      "2025-01-21 01:21:16,023 [INFO]: Analyzing table: Role\n",
      "2025-01-21 01:21:16,029 [INFO]: Analyzing table: Profil\n",
      "2025-01-21 01:21:16,034 [INFO]: Analyzing table: Transport\n",
      "2025-01-21 01:21:16,037 [INFO]: Analyzing table: Aliment\n",
      "2025-01-21 01:21:16,043 [INFO]: Analyzing table: Chauffage\n",
      "2025-01-21 01:21:16,049 [INFO]: Analyzing table: TransportEmission\n",
      "2025-01-21 01:21:16,407 [INFO]: Analyzing table: AlimentationEmission\n",
      "2025-01-21 01:21:16,474 [INFO]: Analyzing table: ChauffageEmission\n",
      "2025-01-21 01:21:16,616 [INFO]: Analyzing table: BEGES\n",
      "2025-01-21 01:21:16,624 [INFO]: Analyzing table: Defi\n",
      "2025-01-21 01:21:16,626 [INFO]: Analyzing table: ProgressionDefi\n",
      "2025-01-21 01:21:16,631 [INFO]: \n",
      "Missing Data Summary:\n",
      "2025-01-21 01:21:16,631 [INFO]: - Role: No missing values.\n",
      "2025-01-21 01:21:16,632 [INFO]: - Profil: No missing values.\n",
      "2025-01-21 01:21:16,633 [INFO]: - Transport: No missing values.\n",
      "2025-01-21 01:21:16,634 [INFO]: - Aliment: No missing values.\n",
      "2025-01-21 01:21:16,635 [INFO]: - Chauffage: No missing values.\n",
      "2025-01-21 01:21:16,637 [INFO]: - TransportEmission: No missing values.\n",
      "2025-01-21 01:21:16,638 [INFO]: - AlimentationEmission: No missing values.\n",
      "2025-01-21 01:21:16,640 [INFO]: - ChauffageEmission: No missing values.\n",
      "2025-01-21 01:21:16,641 [INFO]: - BEGES: No missing values.\n",
      "2025-01-21 01:21:16,641 [INFO]: - Defi: No missing values.\n",
      "2025-01-21 01:21:16,642 [INFO]: - ProgressionDefi: No missing values.\n",
      "2025-01-21 01:21:16,643 [INFO]: \n",
      "Proposed Corrections:\n",
      "2025-01-21 01:21:16,649 [INFO]: Missing data summary saved to 'missing_data_summary.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import logging\n",
    "from typing import Dict\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s]: %(message)s\")\n",
    "\n",
    "# Database configuration\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"ClashOfRse\",\n",
    "    \"user\": \"your_username\",\n",
    "    \"password\": \"your_password\",\n",
    "}\n",
    "\n",
    "def connect_to_db() -> create_engine:\n",
    "    \"\"\"Connect to the PostgreSQL database using SQLAlchemy.\"\"\"\n",
    "    try:\n",
    "        db_url = f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}/{DB_CONFIG['database']}\"\n",
    "        engine = create_engine(db_url)\n",
    "        # Test the connection\n",
    "        engine.connect()\n",
    "        logging.info(\"Successfully connected to the database.\")\n",
    "        return engine\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to connect to the database: {e}\")\n",
    "        raise\n",
    "\n",
    "def fetch_table_data(table_name: str, engine: create_engine) -> pd.DataFrame:\n",
    "    \"\"\"Fetch data from a specific table.\"\"\"\n",
    "    try:\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        df = pd.read_sql_query(query, con=engine)\n",
    "        if df.empty:\n",
    "            logging.warning(f\"Table '{table_name}' is empty.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch data from table '{table_name}': {e}\")\n",
    "        raise\n",
    "\n",
    "def identify_missing_data(engine: create_engine) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"Identify missing data in all tables.\"\"\"\n",
    "    tables = [\"Role\", \"Profil\", \"Transport\", \"Aliment\", \"Chauffage\", \"TransportEmission\",\n",
    "              \"AlimentationEmission\", \"ChauffageEmission\", \"BEGES\", \"Defi\", \"ProgressionDefi\"]\n",
    "    missing_data_summary = {}\n",
    "\n",
    "    for table in tables:\n",
    "        try:\n",
    "            logging.info(f\"Analyzing table: {table}\")\n",
    "            df = fetch_table_data(table, engine)\n",
    "            missing_counts = df.isnull().sum()\n",
    "            missing_data_summary[table] = missing_counts[missing_counts > 0].to_dict()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error analyzing table '{table}': {e}\")\n",
    "    \n",
    "    # Log missing data summary\n",
    "    logging.info(\"\\nMissing Data Summary:\")\n",
    "    for table, missing_columns in missing_data_summary.items():\n",
    "        if missing_columns:\n",
    "            logging.info(f\"- {table}:\")\n",
    "            for column, count in missing_columns.items():\n",
    "                logging.info(f\"    {column}: {count} missing values\")\n",
    "        else:\n",
    "            logging.info(f\"- {table}: No missing values.\")\n",
    "    \n",
    "    return missing_data_summary\n",
    "\n",
    "def propose_corrections(missing_data_summary: Dict[str, Dict[str, int]]) -> Dict[tuple, str]:\n",
    "    \"\"\"Propose corrections for missing data.\"\"\"\n",
    "    corrections = {}\n",
    "\n",
    "    for table, missing_columns in missing_data_summary.items():\n",
    "        for column in missing_columns:\n",
    "            if \"Id\" in column:\n",
    "                corrections[(table, column)] = \"Ensure referenced table is populated and foreign keys are valid.\"\n",
    "            elif \"Nom\" in column or \"Prenom\" in column:\n",
    "                corrections[(table, column)] = \"Generate random names or use placeholders.\"\n",
    "            elif \"CO2\" in column:\n",
    "                corrections[(table, column)] = \"Impute with average or default CO2 values.\"\n",
    "            elif \"Date\" in column:\n",
    "                corrections[(table, column)] = \"Fill with default dates or approximate ranges.\"\n",
    "            elif \"Score\" in column or \"Rank\" in column:\n",
    "                corrections[(table, column)] = \"Impute with median or random valid scores/ranks.\"\n",
    "            else:\n",
    "                corrections[(table, column)] = \"Analyze and determine logical imputation values.\"\n",
    "\n",
    "    # Log proposed corrections\n",
    "    logging.info(\"\\nProposed Corrections:\")\n",
    "    for (table, column), correction in corrections.items():\n",
    "        logging.info(f\"- {table}.{column}: {correction}\")\n",
    "\n",
    "    return corrections\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        engine = connect_to_db()\n",
    "\n",
    "        # Identify missing data\n",
    "        missing_data_summary = identify_missing_data(engine)\n",
    "\n",
    "        # Propose corrections\n",
    "        corrections = propose_corrections(missing_data_summary)\n",
    "\n",
    "        # Optionally save results to a file\n",
    "        pd.DataFrame.from_dict(missing_data_summary, orient='index').to_csv(\"missing_data_summary.csv\")\n",
    "        logging.info(\"Missing data summary saved to 'missing_data_summary.csv'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An error occurred during execution: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'Role' exported to 'Role.csv'.\n",
      "Table 'Profil' exported to 'Profil.csv'.\n",
      "Table 'Transport' exported to 'Transport.csv'.\n",
      "Table 'Aliment' exported to 'Aliment.csv'.\n",
      "Table 'Chauffage' exported to 'Chauffage.csv'.\n",
      "Table 'TransportEmission' exported to 'TransportEmission.csv'.\n",
      "Table 'AlimentationEmission' exported to 'AlimentationEmission.csv'.\n",
      "Table 'ChauffageEmission' exported to 'ChauffageEmission.csv'.\n",
      "Table 'BEGES' exported to 'BEGES.csv'.\n",
      "Table 'Defi' exported to 'Defi.csv'.\n",
      "Table 'ProgressionDefi' exported to 'ProgressionDefi.csv'.\n",
      "All tables have been successfully exported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database configuration\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"ClashOfRse\",\n",
    "    \"user\": \"your_username\",\n",
    "    \"password\": \"your_password\",\n",
    "}\n",
    "\n",
    "def connect_to_db():\n",
    "    \"\"\"Connect to the PostgreSQL database using SQLAlchemy.\"\"\"\n",
    "    db_url = f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}/{DB_CONFIG['database']}\"\n",
    "    return create_engine(db_url)\n",
    "\n",
    "def fetch_and_export_table(table_name, engine):\n",
    "    \"\"\"Fetch data from a specific table and export it to a CSV file.\"\"\"\n",
    "    try:\n",
    "        query = f\"SELECT * FROM {table_name}\"\n",
    "        df = pd.read_sql_query(query, con=engine)\n",
    "        file_name = f\"{table_name}.csv\"\n",
    "        df.to_csv(file_name, index=False)\n",
    "        print(f\"Table '{table_name}' exported to '{file_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while exporting table '{table_name}': {e}\")\n",
    "\n",
    "def export_all_tables():\n",
    "    \"\"\"Export all tables to separate CSV files.\"\"\"\n",
    "    tables = [\n",
    "        \"Role\", \"Profil\", \"Transport\", \"Aliment\", \"Chauffage\",\n",
    "        \"TransportEmission\", \"AlimentationEmission\", \"ChauffageEmission\",\n",
    "        \"BEGES\", \"Defi\", \"ProgressionDefi\"\n",
    "    ]\n",
    "    try:\n",
    "        engine = connect_to_db()\n",
    "        for table in tables:\n",
    "            fetch_and_export_table(table, engine)\n",
    "        print(\"All tables have been successfully exported.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_all_tables()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Debugging Emissions Data ---\n",
      "Transport Emissions Total: 23865.87256959342\n",
      "Alimentation Emissions Total: 1324.0329751981849\n",
      "Heating Emissions Total: 9723.713702753625\n",
      "Total Emissions: 34913.619247545226\n",
      "\n",
      "--- Profile Details ---\n",
      "   profilid  roleid     nom    prenom datedenaissance              email  \\\n",
      "0         1       4  User_1  Prenom_1      1971-03-07  user1@example.com   \n",
      "\n",
      "     entreprise                                  image  distancetravailmaison  \n",
      "0  Entreprise_6  https://example.com/default-image.png               4.510607  \n",
      "\n",
      "--- BEGES Records ---\n",
      "    begesid  profilid   datedebut     datefin     co2total  globalrank\n",
      "0        84         1  2023-02-01  2023-03-03   868.275487           3\n",
      "1        94         1  2023-05-14  2023-06-13   377.990205           3\n",
      "2       115         1  2023-03-07  2023-04-06   552.838263           3\n",
      "3       201         1  2023-01-01  2023-01-30   548.827636           3\n",
      "4       202         1  2023-01-31  2023-03-01   630.396838           3\n",
      "5       203         1  2023-03-02  2023-03-31  1691.786928           3\n",
      "6       204         1  2023-04-01  2023-04-30   703.285873           3\n",
      "7       205         1  2023-05-01  2023-05-30  1326.805176           3\n",
      "8       206         1  2023-05-31  2023-06-29  1277.179748           3\n",
      "9       207         1  2023-06-30  2023-07-29  1782.771440           3\n",
      "10      208         1  2023-07-30  2023-08-28  1946.013639           3\n",
      "11      209         1  2023-08-29  2023-09-27  1583.860412           3\n",
      "12      210         1  2023-09-28  2023-10-27   566.574379           3\n",
      "13      211         1  2023-10-28  2023-11-26  1635.028255           3\n",
      "14      212         1  2023-11-27  2023-12-26   540.347629           3\n",
      "\n",
      "--- Emissions Summary ---\n",
      "Transport CO2: 23865.87256959342 kg\n",
      "Alimentation CO2: 1324.0329751981849 kg\n",
      "Chauffage CO2: 9723.713702753625 kg\n",
      "Total CO2 Emissions: 34913.619247545226 kg\n",
      "\n",
      "--- Challenge Progression ---\n",
      "    progressionid  profilid  defiid  score  definom  \\\n",
      "0               1         1      19     29  Defi_19   \n",
      "1               2         1       2     51   Defi_2   \n",
      "2               3         1      44     92  Defi_44   \n",
      "3               4         1      38     27  Defi_38   \n",
      "4               5         1      35     14  Defi_35   \n",
      "5               6         1       9     23   Defi_9   \n",
      "6               7         1      17     77  Defi_17   \n",
      "7               8         1       7     90   Defi_7   \n",
      "8               9         1       5     11   Defi_5   \n",
      "9              10         1      47     84  Defi_47   \n",
      "10             11         1      11     62  Defi_11   \n",
      "11             12         1      10     81  Defi_10   \n",
      "\n",
      "                     description  objectif  \n",
      "0   Description for challenge 19        37  \n",
      "1    Description for challenge 2        79  \n",
      "2   Description for challenge 44        97  \n",
      "3   Description for challenge 38        97  \n",
      "4   Description for challenge 35        21  \n",
      "5    Description for challenge 9        50  \n",
      "6   Description for challenge 17        86  \n",
      "7    Description for challenge 7        56  \n",
      "8    Description for challenge 5        39  \n",
      "9   Description for challenge 47        61  \n",
      "10  Description for challenge 11        81  \n",
      "11  Description for challenge 10        97  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database configuration\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"ClashOfRse\",\n",
    "    \"user\": \"your_username\",\n",
    "    \"password\": \"your_password\",\n",
    "}\n",
    "\n",
    "def connect_to_db():\n",
    "    \"\"\"Connect to the PostgreSQL database using SQLAlchemy.\"\"\"\n",
    "    db_url = f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}/{DB_CONFIG['database']}\"\n",
    "    return create_engine(db_url)\n",
    "\n",
    "def analyze_user_data(profil_id):\n",
    "    \"\"\"Analyze data for a specific user (ProfilId).\"\"\"\n",
    "    engine = connect_to_db()\n",
    "    \n",
    "    # Fetch profile details\n",
    "    profile_query = f\"SELECT * FROM Profil WHERE ProfilId = {profil_id}\"\n",
    "    profile = pd.read_sql_query(profile_query, con=engine)\n",
    "    \n",
    "    # Fetch BEGES records\n",
    "    beges_query = f\"SELECT * FROM BEGES WHERE ProfilId = {profil_id}\"\n",
    "    beges = pd.read_sql_query(beges_query, con=engine)\n",
    "    \n",
    "    # Fetch emissions data linked to BEGES\n",
    "    transport_query = f\"\"\"\n",
    "        SELECT te.* \n",
    "        FROM TransportEmission te \n",
    "        JOIN BEGES b ON te.BEGESId = b.BEGESId\n",
    "        WHERE b.ProfilId = {profil_id}\n",
    "    \"\"\"\n",
    "    transport_emissions = pd.read_sql_query(transport_query, con=engine)\n",
    "    \n",
    "    alimentation_query = f\"\"\"\n",
    "        SELECT ae.* \n",
    "        FROM AlimentationEmission ae \n",
    "        JOIN BEGES b ON ae.BEGESId = b.BEGESId\n",
    "        WHERE b.ProfilId = {profil_id}\n",
    "    \"\"\"\n",
    "    alimentation_emissions = pd.read_sql_query(alimentation_query, con=engine)\n",
    "    \n",
    "    chauffage_query = f\"\"\"\n",
    "        SELECT ce.* \n",
    "        FROM ChauffageEmission ce \n",
    "        JOIN BEGES b ON ce.BEGESId = b.BEGESId\n",
    "        WHERE b.ProfilId = {profil_id}\n",
    "    \"\"\"\n",
    "    chauffage_emissions = pd.read_sql_query(chauffage_query, con=engine)\n",
    "    \n",
    "    # Summarize emissions data\n",
    "    total_transport_co2 = transport_emissions['co2emistransport'].sum()\n",
    "    total_alimentation_co2 = alimentation_emissions['co2emisalimentation'].sum() if 'co2emisalimentation' in alimentation_emissions else 0\n",
    "    total_chauffage_co2 = chauffage_emissions['co2emischauffage'].sum() if 'co2emischauffage' in chauffage_emissions else 0\n",
    "    total_emissions = total_transport_co2 + total_alimentation_co2 + total_chauffage_co2\n",
    "\n",
    "    # Debug: Log intermediate results\n",
    "    print(\"\\n--- Debugging Emissions Data ---\")\n",
    "    print(\"Transport Emissions Total:\", total_transport_co2)\n",
    "    print(\"Alimentation Emissions Total:\", total_alimentation_co2)\n",
    "    print(\"Heating Emissions Total:\", total_chauffage_co2)\n",
    "    print(\"Total Emissions:\", total_emissions)\n",
    "    \n",
    "    # Print summaries\n",
    "    print(\"\\n--- Profile Details ---\")\n",
    "    print(profile)\n",
    "    \n",
    "    print(\"\\n--- BEGES Records ---\")\n",
    "    print(beges)\n",
    "    \n",
    "    print(\"\\n--- Emissions Summary ---\")\n",
    "    print(f\"Transport CO2: {total_transport_co2} kg\")\n",
    "    print(f\"Alimentation CO2: {total_alimentation_co2} kg\")\n",
    "    print(f\"Chauffage CO2: {total_chauffage_co2} kg\")\n",
    "    print(f\"Total CO2 Emissions: {total_emissions} kg\")\n",
    "    \n",
    "    # Fetch challenge progression\n",
    "    progression_query = f\"\"\"\n",
    "        SELECT pd.*, d.Nom AS DefiNom, d.Description, d.Objectif\n",
    "        FROM ProgressionDefi pd\n",
    "        JOIN Defi d ON pd.DefiId = d.DefiId\n",
    "        WHERE pd.ProfilId = {profil_id}\n",
    "    \"\"\"\n",
    "    progression = pd.read_sql_query(progression_query, con=engine)\n",
    "    print(\"\\n--- Challenge Progression ---\")\n",
    "    print(progression)\n",
    "    \n",
    "    return {\n",
    "        \"profile\": profile,\n",
    "        \"beges\": beges,\n",
    "        \"transport_emissions\": transport_emissions,\n",
    "        \"alimentation_emissions\": alimentation_emissions,\n",
    "        \"chauffage_emissions\": chauffage_emissions,\n",
    "        \"progression\": progression\n",
    "    }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Analyze data for a specific user (replace '1' with the desired ProfilId)\n",
    "    user_data = analyze_user_data(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on datetime64[ns] and object columns for key 'date'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     91\u001b[0m     profil_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Replace with the desired user ID\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m     timeline \u001b[38;5;241m=\u001b[39m \u001b[43mget_emissions_timeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprofil_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     plot_emissions_timeline(timeline)\n",
      "Cell \u001b[1;32mIn[17], line 59\u001b[0m, in \u001b[0;36mget_emissions_timeline\u001b[1;34m(profil_id)\u001b[0m\n\u001b[0;32m     57\u001b[0m timeline[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(transport_emissions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatedebut\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     58\u001b[0m timeline[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransport_co2\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m transport_emissions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransport_co2\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 59\u001b[0m timeline \u001b[38;5;241m=\u001b[39m \u001b[43mtimeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43malimentation_emissions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mouter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatedebut\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m timeline \u001b[38;5;241m=\u001b[39m timeline\u001b[38;5;241m.\u001b[39mmerge(chauffage_emissions, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m'\u001b[39m, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatedebut\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Clean up columns\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Yann\\Desktop\\DEV\\School\\Hackathon_Ynov_2025\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:10832\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m  10813\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m  10814\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m  10815\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10828\u001b[0m     validate: MergeValidate \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m  10829\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m  10830\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m> 10832\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m  10833\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10841\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10842\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m  10846\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Yann\\Desktop\\DEV\\School\\Hackathon_Ynov_2025\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:170\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[0;32m    156\u001b[0m         left_df,\n\u001b[0;32m    157\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\Yann\\Desktop\\DEV\\School\\Hackathon_Ynov_2025\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:807\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[1;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_tolerance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys)\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[0;32m    806\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m--> 807\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Yann\\Desktop\\DEV\\School\\Hackathon_Ynov_2025\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1512\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m-> 1512\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1513\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: You are trying to merge on datetime64[ns] and object columns for key 'date'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Database configuration\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"database\": \"ClashOfRse\",\n",
    "    \"user\": \"your_username\",\n",
    "    \"password\": \"your_password\",\n",
    "}\n",
    "\n",
    "def connect_to_db():\n",
    "    \"\"\"Connect to the PostgreSQL database using SQLAlchemy.\"\"\"\n",
    "    db_url = f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}/{DB_CONFIG['database']}\"\n",
    "    return create_engine(db_url)\n",
    "\n",
    "def get_emissions_timeline(profil_id):\n",
    "    \"\"\"Fetch and combine emissions data over time for a specific user.\"\"\"\n",
    "    engine = connect_to_db()\n",
    "\n",
    "    # Transport Emissions\n",
    "    transport_query = f\"\"\"\n",
    "        SELECT te.datedebut, SUM(te.co2emistransport) AS transport_co2 \n",
    "        FROM TransportEmission te\n",
    "        JOIN BEGES b ON te.BEGESId = b.BEGESId\n",
    "        WHERE b.ProfilId = {profil_id}\n",
    "        GROUP BY te.datedebut\n",
    "        ORDER BY te.datedebut\n",
    "    \"\"\"\n",
    "    transport_emissions = pd.read_sql_query(transport_query, con=engine)\n",
    "    transport_emissions['datedebut'] = pd.to_datetime(transport_emissions['datedebut'])\n",
    "\n",
    "    # Alimentation Emissions\n",
    "    alimentation_query = f\"\"\"\n",
    "        SELECT ae.datedebut, SUM(ae.co2emisalimentation) AS alimentation_co2 \n",
    "        FROM AlimentationEmission ae\n",
    "        JOIN BEGES b ON ae.BEGESId = b.BEGESId\n",
    "        WHERE b.ProfilId = {profil_id}\n",
    "        GROUP BY ae.datedebut\n",
    "        ORDER BY ae.datedebut\n",
    "    \"\"\"\n",
    "    alimentation_emissions = pd.read_sql_query(alimentation_query, con=engine)\n",
    "    alimentation_emissions['datedebut'] = pd.to_datetime(alimentation_emissions['datedebut'])\n",
    "\n",
    "    # Heating Emissions\n",
    "    chauffage_query = f\"\"\"\n",
    "        SELECT ce.datedebut, SUM(ce.co2emischauffage) AS chauffage_co2 \n",
    "        FROM ChauffageEmission ce\n",
    "        JOIN BEGES b ON ce.BEGESId = b.BEGESId\n",
    "        WHERE b.ProfilId = {profil_id}\n",
    "        GROUP BY ce.datedebut\n",
    "        ORDER BY ce.datedebut\n",
    "    \"\"\"\n",
    "    chauffage_emissions = pd.read_sql_query(chauffage_query, con=engine)\n",
    "    chauffage_emissions['datedebut'] = pd.to_datetime(chauffage_emissions['datedebut'])\n",
    "\n",
    "    # Combine data into a single DataFrame\n",
    "    timeline = pd.DataFrame()\n",
    "    timeline['date'] = transport_emissions['datedebut']\n",
    "    timeline['transport_co2'] = transport_emissions['transport_co2']\n",
    "    timeline = timeline.merge(alimentation_emissions, how='outer', left_on='date', right_on='datedebut')\n",
    "    timeline = timeline.merge(chauffage_emissions, how='outer', left_on='date', right_on='datedebut')\n",
    "\n",
    "    # Clean up columns\n",
    "    timeline.drop(columns=['datedebut_x', 'datedebut_y'], inplace=True)\n",
    "    timeline.rename(columns={'alimentation_co2': 'alimentation_co2', 'chauffage_co2': 'chauffage_co2'}, inplace=True)\n",
    "\n",
    "    # Fill missing values with 0 for plotting\n",
    "    timeline.fillna(0, inplace=True)\n",
    "\n",
    "    # Sort by date\n",
    "    timeline.sort_values(by='date', inplace=True)\n",
    "    \n",
    "    return timeline\n",
    "\n",
    "\n",
    "def plot_emissions_timeline(timeline):\n",
    "    \"\"\"Plot the emissions timeline.\"\"\"\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    plt.plot(timeline['date'], timeline['transport_co2'], label='Transport CO2', marker='o')\n",
    "    plt.plot(timeline['date'], timeline['alimentation_co2'], label='Alimentation CO2', marker='s')\n",
    "    plt.plot(timeline['date'], timeline['chauffage_co2'], label='Heating CO2', marker='^')\n",
    "\n",
    "    plt.title('User Emissions Evolution Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('CO2 Emissions (kg)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    profil_id = 1  # Replace with the desired user ID\n",
    "    timeline = get_emissions_timeline(profil_id)\n",
    "    plot_emissions_timeline(timeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.1-cp312-cp312-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.55.3-cp312-cp312-win_amd64.whl.metadata (168 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\yann\\desktop\\dev\\school\\hackathon_ynov_2025\\venv\\lib\\site-packages (from matplotlib) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yann\\desktop\\dev\\school\\hackathon_ynov_2025\\venv\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Using cached pillow-11.1.0-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\yann\\desktop\\dev\\school\\hackathon_ynov_2025\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yann\\desktop\\dev\\school\\hackathon_ynov_2025\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached matplotlib-3.10.0-cp312-cp312-win_amd64.whl (8.0 MB)\n",
      "Using cached contourpy-1.3.1-cp312-cp312-win_amd64.whl (220 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.55.3-cp312-cp312-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.8-cp312-cp312-win_amd64.whl (71 kB)\n",
      "Using cached pillow-11.1.0-cp312-cp312-win_amd64.whl (2.6 MB)\n",
      "Using cached pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.3.1 cycler-0.12.1 fonttools-4.55.3 kiwisolver-1.4.8 matplotlib-3.10.0 pillow-11.1.0 pyparsing-3.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
